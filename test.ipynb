{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [['human', 'interface', 'computer'],\n",
    "['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "['eps', 'user', 'interface', 'system'],\n",
    "['system', 'human', 'system', 'eps'],\n",
    "['user', 'response', 'time'],\n",
    "['trees'],\n",
    "['graph', 'trees'],\n",
    "['graph', 'minors', 'trees'],\n",
    "['graph', 'minors', 'survey']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(corpus[0])# [(0, 1), (1, 1), (2, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (5, 2), (8, 1)],\n",
       " [(3, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(4, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.tfidfmodel.TfidfModel at 0x1050bc210>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus(object):\n",
    "    \"\"\"An interator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = model.wv['king']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hundreds\n",
      "of\n",
      "people\n",
      "have\n",
      "been\n",
      "forced\n",
      "to\n",
      "their\n",
      "homes\n",
      "in\n",
      "the\n",
      "southern\n",
      "new\n",
      "south\n",
      "wales\n",
      "as\n",
      "strong\n",
      "winds\n",
      "today\n",
      "huge\n",
      "towards\n",
      "town\n",
      "hill\n",
      "top\n",
      "blaze\n",
      "near\n",
      "west\n",
      "sydney\n",
      "has\n",
      "highway\n",
      "at\n",
      "about\n",
      "pm\n",
      "aedt\n",
      "weather\n",
      "storm\n",
      "moved\n",
      "east\n",
      "across\n",
      "blue\n",
      "mountains\n",
      "authorities\n",
      "make\n",
      "decision\n",
      "from\n",
      "streets\n",
      "an\n",
      "residents\n",
      "left\n",
      "for\n",
      "nearby\n",
      "rural\n",
      "fire\n",
      "service\n",
      "says\n",
      "conditions\n",
      "which\n",
      "caused\n",
      "now\n",
      "and\n",
      "around\n",
      "are\n",
      "all\n",
      "more\n",
      "than\n",
      "blazes\n",
      "on\n",
      "year\n",
      "eve\n",
      "crews\n",
      "called\n",
      "while\n",
      "few\n",
      "details\n",
      "available\n",
      "this\n",
      "stage\n",
      "it\n",
      "closed\n",
      "both\n",
      "meanwhile\n",
      "is\n",
      "no\n",
      "longer\n",
      "threatening\n",
      "area\n",
      "rain\n",
      "some\n",
      "parts\n",
      "illawarra\n",
      "hunter\n",
      "north\n",
      "coast\n",
      "but\n",
      "bureau\n",
      "done\n",
      "little\n",
      "any\n",
      "hundred\n",
      "fires\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(model.wv.vocab):\n",
    "    if i == 100:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用Gensim存储/加载模型\n",
    "import tempfile\n",
    "\n",
    "#with tempfile.NamedTemporaryFile(prefix='gensim-model-', dir='.', delete=False) as tmp:\n",
    "with tempfile.NamedTemporaryFile(prefix='gensim-model-', delete=False) as tmp:\n",
    "    temporary_filepath = tmp.name\n",
    "    model.save(temporary_filepath)\n",
    "    #\n",
    "    # The model is now safely stored in the filepath.\n",
    "    # You can copy it to other machines, share it with others, etc.\n",
    "    #\n",
    "    # To load a saved model:\n",
    "    #\n",
    "    new_model = gensim.models.Word2Vec.load(temporary_filepath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hongshibo/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `accuracy` (Method will be removed in 4.0.0, use self.wv.evaluate_word_analogies() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'section': 'capital-common-countries',\n",
       "  'correct': [],\n",
       "  'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'),\n",
       "   ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n",
       "   ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN')]},\n",
       " {'section': 'capital-world',\n",
       "  'correct': [],\n",
       "  'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE')]},\n",
       " {'section': 'currency', 'correct': [], 'incorrect': []},\n",
       " {'section': 'city-in-state', 'correct': [], 'incorrect': []},\n",
       " {'section': 'family',\n",
       "  'correct': [('HIS', 'HER', 'HE', 'SHE')],\n",
       "  'incorrect': [('HE', 'SHE', 'HIS', 'HER'),\n",
       "   ('HE', 'SHE', 'MAN', 'WOMAN'),\n",
       "   ('HIS', 'HER', 'MAN', 'WOMAN'),\n",
       "   ('MAN', 'WOMAN', 'HE', 'SHE'),\n",
       "   ('MAN', 'WOMAN', 'HIS', 'HER')]},\n",
       " {'section': 'gram1-adjective-to-adverb', 'correct': [], 'incorrect': []},\n",
       " {'section': 'gram2-opposite', 'correct': [], 'incorrect': []},\n",
       " {'section': 'gram3-comparative',\n",
       "  'correct': [],\n",
       "  'incorrect': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
       "   ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
       "   ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
       "   ('GOOD', 'BETTER', 'SMALL', 'SMALLER'),\n",
       "   ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
       "   ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
       "   ('GREAT', 'GREATER', 'SMALL', 'SMALLER'),\n",
       "   ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
       "   ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
       "   ('LONG', 'LONGER', 'SMALL', 'SMALLER'),\n",
       "   ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
       "   ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
       "   ('LOW', 'LOWER', 'SMALL', 'SMALLER'),\n",
       "   ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
       "   ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
       "   ('LOW', 'LOWER', 'LONG', 'LONGER'),\n",
       "   ('SMALL', 'SMALLER', 'GOOD', 'BETTER'),\n",
       "   ('SMALL', 'SMALLER', 'GREAT', 'GREATER'),\n",
       "   ('SMALL', 'SMALLER', 'LONG', 'LONGER'),\n",
       "   ('SMALL', 'SMALLER', 'LOW', 'LOWER')]},\n",
       " {'section': 'gram4-superlative',\n",
       "  'correct': [],\n",
       "  'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
       "   ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
       "   ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
       "   ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
       "   ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
       "   ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
       "   ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
       "   ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
       "   ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
       "   ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
       "   ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
       "   ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')]},\n",
       " {'section': 'gram5-present-participle',\n",
       "  'correct': [],\n",
       "  'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
       "   ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
       "   ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
       "   ('GO', 'GOING', 'SAY', 'SAYING'),\n",
       "   ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
       "   ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
       "   ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
       "   ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
       "   ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
       "   ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
       "   ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
       "   ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
       "   ('SAY', 'SAYING', 'GO', 'GOING'),\n",
       "   ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
       "   ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
       "   ('SAY', 'SAYING', 'RUN', 'RUNNING')]},\n",
       " {'section': 'gram6-nationality-adjective',\n",
       "  'correct': [('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN')],\n",
       "  'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
       "   ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
       "   ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
       "   ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'),\n",
       "   ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
       "   ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
       "   ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'),\n",
       "   ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
       "   ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
       "   ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'),\n",
       "   ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
       "   ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
       "   ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
       "   ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'),\n",
       "   ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'),\n",
       "   ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'),\n",
       "   ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'),\n",
       "   ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
       "   ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n",
       "   ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE')]},\n",
       " {'section': 'gram7-past-tense',\n",
       "  'correct': [],\n",
       "  'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'),\n",
       "   ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
       "   ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
       "   ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
       "   ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
       "   ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
       "   ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
       "   ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
       "   ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
       "   ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
       "   ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
       "   ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
       "   ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
       "   ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
       "   ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
       "   ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'SAYING', 'SAID')]},\n",
       " {'section': 'gram8-plural',\n",
       "  'correct': [],\n",
       "  'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
       "   ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
       "   ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
       "   ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'),\n",
       "   ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'),\n",
       "   ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
       "   ('CAR', 'CARS', 'MAN', 'MEN'),\n",
       "   ('CAR', 'CARS', 'ROAD', 'ROADS'),\n",
       "   ('CAR', 'CARS', 'WOMAN', 'WOMEN'),\n",
       "   ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
       "   ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
       "   ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'),\n",
       "   ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'),\n",
       "   ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
       "   ('MAN', 'MEN', 'ROAD', 'ROADS'),\n",
       "   ('MAN', 'MEN', 'WOMAN', 'WOMEN'),\n",
       "   ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('MAN', 'MEN', 'CAR', 'CARS'),\n",
       "   ('MAN', 'MEN', 'CHILD', 'CHILDREN'),\n",
       "   ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'),\n",
       "   ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'),\n",
       "   ('ROAD', 'ROADS', 'CAR', 'CARS'),\n",
       "   ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'),\n",
       "   ('ROAD', 'ROADS', 'MAN', 'MEN'),\n",
       "   ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('WOMAN', 'WOMEN', 'CAR', 'CARS'),\n",
       "   ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'),\n",
       "   ('WOMAN', 'WOMEN', 'MAN', 'MEN'),\n",
       "   ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]},\n",
       " {'section': 'gram9-plural-verbs', 'correct': [], 'incorrect': []},\n",
       " {'section': 'total',\n",
       "  'correct': [('HIS', 'HER', 'HE', 'SHE'),\n",
       "   ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN')],\n",
       "  'incorrect': [('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('CANBERRA', 'AUSTRALIA', 'PARIS', 'FRANCE'),\n",
       "   ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n",
       "   ('KABUL', 'AFGHANISTAN', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('PARIS', 'FRANCE', 'CANBERRA', 'AUSTRALIA'),\n",
       "   ('PARIS', 'FRANCE', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('CANBERRA', 'AUSTRALIA', 'KABUL', 'AFGHANISTAN'),\n",
       "   ('KABUL', 'AFGHANISTAN', 'PARIS', 'FRANCE'),\n",
       "   ('HE', 'SHE', 'HIS', 'HER'),\n",
       "   ('HE', 'SHE', 'MAN', 'WOMAN'),\n",
       "   ('HIS', 'HER', 'MAN', 'WOMAN'),\n",
       "   ('MAN', 'WOMAN', 'HE', 'SHE'),\n",
       "   ('MAN', 'WOMAN', 'HIS', 'HER'),\n",
       "   ('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
       "   ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
       "   ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
       "   ('GOOD', 'BETTER', 'SMALL', 'SMALLER'),\n",
       "   ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
       "   ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
       "   ('GREAT', 'GREATER', 'SMALL', 'SMALLER'),\n",
       "   ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
       "   ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
       "   ('LONG', 'LONGER', 'SMALL', 'SMALLER'),\n",
       "   ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
       "   ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
       "   ('LOW', 'LOWER', 'SMALL', 'SMALLER'),\n",
       "   ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
       "   ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
       "   ('LOW', 'LOWER', 'LONG', 'LONGER'),\n",
       "   ('SMALL', 'SMALLER', 'GOOD', 'BETTER'),\n",
       "   ('SMALL', 'SMALLER', 'GREAT', 'GREATER'),\n",
       "   ('SMALL', 'SMALLER', 'LONG', 'LONGER'),\n",
       "   ('SMALL', 'SMALLER', 'LOW', 'LOWER'),\n",
       "   ('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
       "   ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
       "   ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
       "   ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
       "   ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
       "   ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
       "   ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
       "   ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
       "   ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
       "   ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
       "   ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
       "   ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'),\n",
       "   ('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
       "   ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
       "   ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
       "   ('GO', 'GOING', 'SAY', 'SAYING'),\n",
       "   ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
       "   ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
       "   ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
       "   ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
       "   ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
       "   ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
       "   ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
       "   ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
       "   ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
       "   ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
       "   ('SAY', 'SAYING', 'GO', 'GOING'),\n",
       "   ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
       "   ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
       "   ('SAY', 'SAYING', 'RUN', 'RUNNING'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'JAPAN', 'JAPANESE'),\n",
       "   ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
       "   ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
       "   ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
       "   ('FRANCE', 'FRENCH', 'JAPAN', 'JAPANESE'),\n",
       "   ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
       "   ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
       "   ('INDIA', 'INDIAN', 'JAPAN', 'JAPANESE'),\n",
       "   ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
       "   ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
       "   ('ISRAEL', 'ISRAELI', 'JAPAN', 'JAPANESE'),\n",
       "   ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
       "   ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
       "   ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
       "   ('JAPAN', 'JAPANESE', 'SWITZERLAND', 'SWISS'),\n",
       "   ('JAPAN', 'JAPANESE', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('JAPAN', 'JAPANESE', 'FRANCE', 'FRENCH'),\n",
       "   ('JAPAN', 'JAPANESE', 'INDIA', 'INDIAN'),\n",
       "   ('JAPAN', 'JAPANESE', 'ISRAEL', 'ISRAELI'),\n",
       "   ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
       "   ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
       "   ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n",
       "   ('SWITZERLAND', 'SWISS', 'JAPAN', 'JAPANESE'),\n",
       "   ('GOING', 'WENT', 'PAYING', 'PAID'),\n",
       "   ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
       "   ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
       "   ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
       "   ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
       "   ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
       "   ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
       "   ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
       "   ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
       "   ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
       "   ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
       "   ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
       "   ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
       "   ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
       "   ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
       "   ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
       "   ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
       "   ('TAKING', 'TOOK', 'SAYING', 'SAID'),\n",
       "   ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
       "   ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
       "   ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
       "   ('BUILDING', 'BUILDINGS', 'ROAD', 'ROADS'),\n",
       "   ('BUILDING', 'BUILDINGS', 'WOMAN', 'WOMEN'),\n",
       "   ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
       "   ('CAR', 'CARS', 'MAN', 'MEN'),\n",
       "   ('CAR', 'CARS', 'ROAD', 'ROADS'),\n",
       "   ('CAR', 'CARS', 'WOMAN', 'WOMEN'),\n",
       "   ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
       "   ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
       "   ('CHILD', 'CHILDREN', 'ROAD', 'ROADS'),\n",
       "   ('CHILD', 'CHILDREN', 'WOMAN', 'WOMEN'),\n",
       "   ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
       "   ('MAN', 'MEN', 'ROAD', 'ROADS'),\n",
       "   ('MAN', 'MEN', 'WOMAN', 'WOMEN'),\n",
       "   ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('MAN', 'MEN', 'CAR', 'CARS'),\n",
       "   ('MAN', 'MEN', 'CHILD', 'CHILDREN'),\n",
       "   ('ROAD', 'ROADS', 'WOMAN', 'WOMEN'),\n",
       "   ('ROAD', 'ROADS', 'BUILDING', 'BUILDINGS'),\n",
       "   ('ROAD', 'ROADS', 'CAR', 'CARS'),\n",
       "   ('ROAD', 'ROADS', 'CHILD', 'CHILDREN'),\n",
       "   ('ROAD', 'ROADS', 'MAN', 'MEN'),\n",
       "   ('WOMAN', 'WOMEN', 'BUILDING', 'BUILDINGS'),\n",
       "   ('WOMAN', 'WOMEN', 'CAR', 'CARS'),\n",
       "   ('WOMAN', 'WOMEN', 'CHILD', 'CHILDREN'),\n",
       "   ('WOMAN', 'WOMEN', 'MAN', 'MEN'),\n",
       "   ('WOMAN', 'WOMEN', 'ROAD', 'ROADS')]}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.accuracy('./questions-words.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hongshibo/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `evaluate_word_pairs` (Method will be removed in 4.0.0, use self.wv.evaluate_word_pairs() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((0.2924548955641028, 0.02335871530659555),\n",
       " SpearmanrResult(correlation=0.14481033853531997, pvalue=0.26962302127073107),\n",
       " 83.0028328611898)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_word_pairs(datapath('wordsim353.tsv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hongshibo/anaconda3/envs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "#model = gensim.models.Word2Vec.load(temporary_filepath)\n",
    "more_sentences = [\n",
    "    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
    "     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences']\n",
    "]\n",
    "new_model.build_vocab(more_sentences, update=True)\n",
    "new_model.train(more_sentences, total_examples=model.corpus_count, epochs=model.iter)\n",
    "\n",
    "# cleaning up temporary file\n",
    "import os\n",
    "os.remove(temporary_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1349074.625\n"
     ]
    }
   ],
   "source": [
    "# instantiating and training the Word2Vec model\n",
    "new_model = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=1,\n",
    "    size=200,\n",
    "    compute_loss=True,\n",
    "    hs=0,\n",
    "    sg=1,\n",
    "    seed=42,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "# getting the training loss value\n",
    "training_loss = new_model.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "\n",
    "import gensim.models.word2vec\n",
    "import gensim.downloader as api\n",
    "import smart_open\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "def head(path, size):\n",
    "    with smart_open.open(path) as fin:\n",
    "        return io.StringIO(fin.read(size))\n",
    "\n",
    "\n",
    "def generate_input_data():\n",
    "    lee_path = datapath('lee_background.cor')\n",
    "    ls = gensim.models.word2vec.LineSentence(lee_path)\n",
    "    ls.name = '25kB'\n",
    "    yield ls\n",
    "\n",
    "    #text8_path = api.load('text8').fn\n",
    "    text8_path = './text8'\n",
    "    labels = ('1MB', '10MB', '50MB', '100MB')\n",
    "    sizes = (1024 ** 2, 10 * 1024 ** 2, 50 * 1024 ** 2, 100 * 1024 ** 2)\n",
    "    for l, s in zip(labels, sizes):\n",
    "        ls = gensim.models.word2vec.LineSentence(head(text8_path, s))\n",
    "        ls.name = l\n",
    "        yield ls\n",
    "\n",
    "\n",
    "input_data = list(generate_input_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1580959785.316521"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec model #0: {'train_data': '25kB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 0.4518406391143799, 'train_time_std': 0.022874163230956316}\n",
      "Word2vec model #1: {'train_data': '25kB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 0.4331976572672526, 'train_time_std': 0.005989090960137032}\n",
      "Word2vec model #2: {'train_data': '25kB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 0.5073786576588949, 'train_time_std': 0.010935730672756588}\n",
      "Word2vec model #3: {'train_data': '25kB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 0.5187516212463379, 'train_time_std': 0.023947410336239965}\n",
      "Word2vec model #4: {'train_data': '25kB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 0.6347042719523112, 'train_time_std': 0.013111959485647915}\n",
      "Word2vec model #5: {'train_data': '25kB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 0.6131624380747477, 'train_time_std': 0.009264342873096857}\n",
      "Word2vec model #6: {'train_data': '25kB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 0.8810825347900391, 'train_time_std': 0.005414333629019884}\n",
      "Word2vec model #7: {'train_data': '25kB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 0.874428908030192, 'train_time_std': 0.005421062106321178}\n",
      "Word2vec model #8: {'train_data': '1MB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 1.0484381516774495, 'train_time_std': 0.015107804015704007}\n",
      "Word2vec model #9: {'train_data': '1MB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 1.0258948802947998, 'train_time_std': 0.003919034362487675}\n",
      "Word2vec model #10: {'train_data': '1MB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 1.4029061794281006, 'train_time_std': 0.02009606185026894}\n",
      "Word2vec model #11: {'train_data': '1MB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 1.4366389910380046, 'train_time_std': 0.04739970679042735}\n",
      "Word2vec model #12: {'train_data': '1MB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 1.835463285446167, 'train_time_std': 0.03086220579948058}\n",
      "Word2vec model #13: {'train_data': '1MB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 1.8992306391398113, 'train_time_std': 0.019237253249805887}\n",
      "Word2vec model #14: {'train_data': '1MB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 3.042510906855265, 'train_time_std': 0.1079331231424832}\n",
      "Word2vec model #15: {'train_data': '1MB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 2.90129017829895, 'train_time_std': 0.01981206848875696}\n",
      "Word2vec model #16: {'train_data': '10MB', 'compute_loss': True, 'sg': 0, 'hs': 0, 'train_time_mean': 8.162419319152832, 'train_time_std': 0.11586976375546888}\n",
      "Word2vec model #17: {'train_data': '10MB', 'compute_loss': False, 'sg': 0, 'hs': 0, 'train_time_mean': 8.016794284184774, 'train_time_std': 0.14249551367572652}\n",
      "Word2vec model #18: {'train_data': '10MB', 'compute_loss': True, 'sg': 0, 'hs': 1, 'train_time_mean': 12.315765619277954, 'train_time_std': 0.069032696937389}\n",
      "Word2vec model #19: {'train_data': '10MB', 'compute_loss': False, 'sg': 0, 'hs': 1, 'train_time_mean': 12.091941595077515, 'train_time_std': 0.029473481566438718}\n",
      "Word2vec model #20: {'train_data': '10MB', 'compute_loss': True, 'sg': 1, 'hs': 0, 'train_time_mean': 18.220116933186848, 'train_time_std': 0.7349548111943144}\n",
      "Word2vec model #21: {'train_data': '10MB', 'compute_loss': False, 'sg': 1, 'hs': 0, 'train_time_mean': 17.55343238512675, 'train_time_std': 0.10679304662339187}\n",
      "Word2vec model #22: {'train_data': '10MB', 'compute_loss': True, 'sg': 1, 'hs': 1, 'train_time_mean': 34.2878364721934, 'train_time_std': 1.128509431352144}\n",
      "Word2vec model #23: {'train_data': '10MB', 'compute_loss': False, 'sg': 1, 'hs': 1, 'train_time_mean': 33.296786069869995, 'train_time_std': 0.590840073945847}\n",
      "   train_data  compute_loss  sg  hs  train_time_mean  train_time_std\n",
      "4        25kB          True   1   0         0.634704        0.013112\n",
      "5        25kB         False   1   0         0.613162        0.009264\n",
      "6        25kB          True   1   1         0.881083        0.005414\n",
      "7        25kB         False   1   1         0.874429        0.005421\n",
      "0        25kB          True   0   0         0.451841        0.022874\n",
      "1        25kB         False   0   0         0.433198        0.005989\n",
      "2        25kB          True   0   1         0.507379        0.010936\n",
      "3        25kB         False   0   1         0.518752        0.023947\n",
      "12        1MB          True   1   0         1.835463        0.030862\n",
      "13        1MB         False   1   0         1.899231        0.019237\n",
      "14        1MB          True   1   1         3.042511        0.107933\n",
      "15        1MB         False   1   1         2.901290        0.019812\n",
      "8         1MB          True   0   0         1.048438        0.015108\n",
      "9         1MB         False   0   0         1.025895        0.003919\n",
      "10        1MB          True   0   1         1.402906        0.020096\n",
      "11        1MB         False   0   1         1.436639        0.047400\n",
      "20       10MB          True   1   0        18.220117        0.734955\n",
      "21       10MB         False   1   0        17.553432        0.106793\n",
      "22       10MB          True   1   1        34.287836        1.128509\n",
      "23       10MB         False   1   1        33.296786        0.590840\n",
      "16       10MB          True   0   0         8.162419        0.115870\n",
      "17       10MB         False   0   0         8.016794        0.142496\n",
      "18       10MB          True   0   1        12.315766        0.069033\n",
      "19       10MB         False   0   1        12.091942        0.029473\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_time_values = []\n",
    "seed_val = 42\n",
    "sg_values = [0, 1]\n",
    "hs_values = [0, 1]\n",
    "\n",
    "fast = True\n",
    "if fast:\n",
    "    input_data_subset = input_data[:3]\n",
    "else:\n",
    "    input_data_subset = input_data\n",
    "\n",
    "\n",
    "for data in input_data_subset:\n",
    "    for sg_val in sg_values:\n",
    "        for hs_val in hs_values:\n",
    "            for loss_flag in [True, False]:\n",
    "                time_taken_list = []\n",
    "                for i in range(3):\n",
    "                    start_time = time.time()\n",
    "                    w2v_model = gensim.models.Word2Vec(\n",
    "                        data,\n",
    "                        compute_loss=loss_flag,\n",
    "                        sg=sg_val,\n",
    "                        hs=hs_val,\n",
    "                        seed=seed_val,\n",
    "                    )\n",
    "                    time_taken_list.append(time.time() - start_time)\n",
    "\n",
    "                time_taken_list = np.array(time_taken_list)\n",
    "                time_mean = np.mean(time_taken_list)\n",
    "                time_std = np.std(time_taken_list)\n",
    "\n",
    "                model_result = {\n",
    "                    'train_data': data.name,\n",
    "                    'compute_loss': loss_flag,\n",
    "                    'sg': sg_val,\n",
    "                    'hs': hs_val,\n",
    "                    'train_time_mean': time_mean,\n",
    "                    'train_time_std': time_std,\n",
    "                }\n",
    "                print(\"Word2vec model #%i: %s\" % (len(train_time_values), model_result))\n",
    "                train_time_values.append(model_result)\n",
    "\n",
    "train_times_table = pd.DataFrame(train_time_values)\n",
    "train_times_table = train_times_table.sort_values(\n",
    "    by=['train_data', 'sg', 'hs', 'compute_loss'],\n",
    "    ascending=[False, False, True, False],\n",
    ")\n",
    "print(train_times_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-02-06 11:41:21,410 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the [('on', 0.9999138116836548), ('in', 0.9999097585678101), ('an', 0.9999094009399414), ('his', 0.9999088048934937), ('four', 0.9999042749404907), ('of', 0.9999032020568848), ('police', 0.9999014139175415), ('after', 0.9999003410339355), ('and', 0.9998999238014221), ('by', 0.9998997449874878)]\n",
      "to [('but', 0.9999513626098633), ('from', 0.9999513626098633), ('and', 0.9999504089355469), ('their', 0.9999456405639648), ('will', 0.9999454021453857), ('one', 0.9999434947967529), ('for', 0.9999432563781738), ('would', 0.9999431371688843), ('which', 0.9999394416809082), ('that', 0.9999392628669739)]\n",
      "of [('on', 0.99993896484375), ('with', 0.9999374151229858), ('and', 0.9999373555183411), ('into', 0.9999346733093262), ('has', 0.9999337196350098), ('from', 0.9999333620071411), ('their', 0.9999324679374695), ('at', 0.9999321699142456), ('in', 0.9999297857284546), ('us', 0.9999282360076904)]\n"
     ]
    }
   ],
   "source": [
    "# re-enable logging\n",
    "logging.root.level = logging.INFO\n",
    "\n",
    "most_similars_precalc = {word : model.wv.most_similar(word) for word in model.wv.index2word}\n",
    "for i, (key, value) in enumerate(most_similars_precalc.items()):\n",
    "    if i == 3:\n",
    "        break\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('despite', 0.9978067874908447), ('several', 0.9977720379829407), ('later', 0.9977660179138184), ('more', 0.9977366328239441), ('thought', 0.9977360367774963), ('rates', 0.9977327585220337), ('plans', 0.9977326393127441), ('years', 0.9977192878723145), ('should', 0.9977146983146667), ('leading', 0.9977137446403503)]\n",
      "[('up', 0.9997690916061401), ('just', 0.9997566938400269), ('first', 0.9997547268867493), ('today', 0.9997521638870239), ('into', 0.9997493624687195), ('their', 0.9997484683990479), ('around', 0.999748170375824), ('or', 0.999748170375824), ('died', 0.9997475147247314), ('but', 0.9997475147247314)]\n",
      "[('by', 0.9999526143074036), ('for', 0.9999512434005737), ('from', 0.9999502301216125), ('australian', 0.9999501705169678), ('but', 0.9999493360519409), ('with', 0.9999491572380066), ('and', 0.9999468326568604), ('also', 0.9999464154243469), ('on', 0.9999462366104126), ('today', 0.9999460577964783)]\n",
      "[('from', 0.999940037727356), ('and', 0.9999386072158813), ('three', 0.9999359250068665), ('on', 0.9999356269836426), ('but', 0.9999347925186157), ('for', 0.999934196472168), ('as', 0.9999341368675232), ('at', 0.999933660030365), ('with', 0.9999305605888367), ('two', 0.9999301433563232)]\n",
      "0.0015687942504882812\n"
     ]
    }
   ],
   "source": [
    "words = ['voted', 'few', 'their', 'around']\n",
    "start = time.time()\n",
    "for word in words:\n",
    "    result = model.wv.most_similar(word)\n",
    "    print(result)\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('despite', 0.9978067874908447), ('several', 0.9977720379829407), ('later', 0.9977660179138184), ('more', 0.9977366328239441), ('thought', 0.9977360367774963), ('rates', 0.9977327585220337), ('plans', 0.9977326393127441), ('years', 0.9977192878723145), ('should', 0.9977146983146667), ('leading', 0.9977137446403503)]\n",
      "[('up', 0.9997690916061401), ('just', 0.9997566938400269), ('first', 0.9997547268867493), ('today', 0.9997521638870239), ('into', 0.9997493624687195), ('their', 0.9997484683990479), ('around', 0.999748170375824), ('or', 0.999748170375824), ('died', 0.9997475147247314), ('but', 0.9997475147247314)]\n",
      "[('by', 0.9999526143074036), ('for', 0.9999512434005737), ('from', 0.9999502301216125), ('australian', 0.9999501705169678), ('but', 0.9999493360519409), ('with', 0.9999491572380066), ('and', 0.9999468326568604), ('also', 0.9999464154243469), ('on', 0.9999462366104126), ('today', 0.9999460577964783)]\n",
      "[('from', 0.999940037727356), ('and', 0.9999386072158813), ('three', 0.9999359250068665), ('on', 0.9999356269836426), ('but', 0.9999347925186157), ('for', 0.999934196472168), ('as', 0.9999341368675232), ('at', 0.999933660030365), ('with', 0.9999305605888367), ('two', 0.9999301433563232)]\n",
      "0.0005488395690917969\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for word in words:\n",
    "    if 'voted' in most_similars_precalc:\n",
    "        result = most_similars_precalc[word]\n",
    "        print(result)\n",
    "    else:\n",
    "        result = model.wv.most_similar(word)\n",
    "        most_similars_precalc[word] = result\n",
    "        print(result)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
